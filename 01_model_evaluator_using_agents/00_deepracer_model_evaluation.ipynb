{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93ddf82",
   "metadata": {},
   "source": [
    "# 使用 Mistral LLM 的 AWS DeepRacer 模型评估器\n",
    "\n",
    "> _此 notebook 可以在 SageMaker Notebook Instance 的 **`conda_python3`** 内核中正常工作_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f2fd3",
   "metadata": {},
   "source": [
    "本次动手实践研讨会演示了如何使用 Amazon Bedrock 结合 Mistral Large（一种大型语言模型 LLM）以及 LangChain 库来构建对话式智能体。该智能体旨在提供有关 AWS DeepRacer 模型和训练的见解和建议。\n",
    "\n",
    "研讨会展示了如何：\n",
    "\n",
    "* 创建自定义 LangChain 工具，让智能体能够与 AWS DeepRacer 服务 API 进行交互。这包括列出可用的模型、下载模型工件以及提取模型元数据，如训练数据和奖励函数。\n",
    "\n",
    "* 在 LangChain 中初始化一个 ReAct 智能体，并为其提供自定义工具。智能体可以根据用户的问题来推理调用哪些工具。\n",
    "\n",
    "* 使用提示技术，如少样本学习（few-shot learning），通过几个示例来提高智能体的推理能力。\n",
    "\n",
    "* 如果智能体的响应与预期格式不匹配，可以优雅地处理错误。\n",
    "\n",
    "* 利用自定义工具，使智能体能够提供有关 AWS DeepRacer 模型的训练数据、超参数、奖励函数等方面的见解。\n",
    "\n",
    "通过本次动手实践研讨会，与会者将能够使用 LLM 构建对话式智能体，通过自定义接口与 AWS 服务集成。关键要点是能够使用工具扩展智能体的能力、设计模块化智能体以及应用提示技术来改善推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c67be5",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "\n",
    "在运行本 notebook 的其余部分之前，您需要安装必要的库，并确保与 Amazon Bedrock 和 AWS DeepRacer 服务 API 建立了有效的连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e09649-99ba-4fb7-8304-7bf03cce5f11",
   "metadata": {},
   "source": [
    "对于本实验，我们需要一些额外的依赖项：\n",
    "- Boto3，AWS 的 Python SDK\n",
    "- ChromaDB，用于存储向量嵌入\n",
    "- LangChain，用于开发由语言模型驱动的应用程序的框架\n",
    "- PyYAML，YAML 解析器\n",
    "\n",
    "⚠️ 您将看到 pip 依赖项错误，您可以安全地忽略这些错误。⚠️\n",
    "\n",
    "忽略错误：pip 的依赖项解析器目前没有考虑所有已安装的包。此行为是以下依赖项冲突的根源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005683fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 安装所需的Python包\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Langchain and associated utilities change frequently\n",
    "# Here, we pin the specific versions we want to use\n",
    "%pip install langchain-core==0.2.11\n",
    "%pip install langchain-aws==0.1.9\n",
    "%pip install langchain==0.2.6\n",
    "%pip install langchainhub==0.1.20\n",
    "%pip install langchain_chroma==0.1.2\n",
    "%pip install chromadb==0.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d4e99",
   "metadata": {},
   "source": [
    "### 添加工具库\n",
    "\n",
    "工具库包含用于与 AWS DeepRacer 服务 API 交互、从 DeepRacer 模型中提取相关信息等的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296adcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "module_path = \"./utils\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from utils import print_ww, deepracer_local\n",
    "\n",
    "# Use importlib to force reloading of local helpfer functions\n",
    "# (useful for debugging when updating these functions, as it avoids the need\n",
    "# to restart the notebook kernel)\n",
    "importlib.reload(deepracer_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd2aac-354f-4b8e-bd03-c8bc154b00c8",
   "metadata": {},
   "source": [
    "本 notebook 还需要访问已作为研讨会设置的一部分部署到所使用的 AWS 账户中的 AWS 基础设施。AWS 基础设施的设置是使用 CloudFormation 完成的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7413c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DeepRacer 服务连接\n",
    "\n",
    "注意：我们将使用此 Jupyter 笔记本本地存储的模型，而不是直接连接到 DeepRacer 服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab71d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# 辅助函数 #\n",
    "###########\n",
    "def translation_format(out):\n",
    "    out = out.strip()\n",
    "\n",
    "    # Perform some parsing (to remove \"Chinese: \" from the start of the model's output)\n",
    "    if out.lower().startswith('chinese:') or out.lower().startswith('中文:'):\n",
    "        out = ':'.join(out.split(':')[1:]).strip()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31637cf2-c7be-4ead-bf83-922f4840a76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 列出可用的模型（来自deepracer_models文件夹）\n",
    "models = deepracer_local.list_models()\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a441225",
   "metadata": {},
   "source": [
    "### Bedrock 服务连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccf10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 连接到Bedrock\n",
    "import boto3\n",
    "bedrock_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfc903",
   "metadata": {},
   "source": [
    "### LLM 设置\n",
    "\n",
    "在本实验中，我们将使用以下 LLM：\n",
    "- Mistral Large，用于推理和行动（ReAct）智能体，以及构建 DeepRacer 问答 LLM 工具。\n",
    "- Amazon Titan 嵌入模型，允许 DeepRacer 问答 LLM 工具在 DeepRacer 文档上进行语义搜索，以提高响应质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa850cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 配置 Mistral (LLM) 和 Titan（Embeddings）\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "bedrock_llm_client = BedrockLLM(\n",
    "    model_id=\"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    #model_id=\"mistral.mistral-large-2402-v1:0\",\n",
    "    client=bedrock_client,\n",
    "    model_kwargs={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "bedrock_embeddings_client = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b5c7f-3a7e-44a3-a665-5fceec7b12f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个新链(LangChain的chain)，用于提供从英语到中文的翻译\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''\n",
    "You are an expert translation bot, with a vast knowledge of human languages.\n",
    "Your task is to translate English content after <<<>>> into Chinese.\n",
    "\n",
    "###\n",
    "Here are some examples:\n",
    "\n",
    "English: Hello, friend\n",
    "Chinese: 你好朋友\n",
    "\n",
    "English: Today, we will learn about DeepRacer\n",
    "Chinese: 今天，我们来学习下DeepRacer\n",
    "###\n",
    "\n",
    "<<<\n",
    "English: {user_input}\n",
    ">>>\n",
    "\n",
    "Do not translate technical terms or product names.\n",
    "Your output is just a direct translation of the English content, and NOTHING ELSE.\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "translation_chain = prompt | bedrock_llm_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78b206-40d4-4441-ad1b-15929808cbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对新链进行一些测试，以确保其按预期工作\n",
    "inputs = [\n",
    "    \"Hello, my name is George\",\n",
    "    \"Let's learn about how the reward function impacts DeepRacer model performance\",\n",
    "    \"Important DeepRacer reward function parameters include 'speed' and 'steering_angle'\",\n",
    "    \"What do you think about DeepRacer?\"\n",
    "]\n",
    "\n",
    "for i in inputs:\n",
    "    print(f'= Original text =\\n {i}')\n",
    "    out = translation_chain.invoke({\"user_input\": i})\n",
    "    out = translation_format(out) # Format output nicely\n",
    "    print(f'= Translation =\\n {out.strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384666fd",
   "metadata": {},
   "source": [
    "## 为 LLM 提供更新的 AWS DeepRacer 知识\n",
    "\n",
    "Mistral Large 对 AWS DeepRacer 有一些先前的了解,但这些知识并没有与最新版本的 AWS DeepRacer 开发者指南保持同步。为了让模型了解最新的内容,我们可以创建一个利用检索增强生成(RAG)的 LangChain 链。\n",
    "\n",
    "在本次工作坊中,我们不会详细介绍 RAG 的工作原理,更多信息请参考:\n",
    "- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "- [Bedrock workshop](https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/03_QuestionAnswering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b451b",
   "metadata": {},
   "source": [
    "### AWS DeepRacer 知识库工具\n",
    "\n",
    "这个工具是另一个专门用于回答 AWS DeepRacer 相关问题的 LangChain 链。该链使用检索增强生成(RAG)来获取与所问问题相关的文档,为回答 AWS DeepRacer 相关问题提供更好的基础。\n",
    "\n",
    "AWS DeepRacer 开发者指南被加载并分割成块,然后转换为嵌入向量。\n",
    "\n",
    "![](./images/Embeddings_lang.png)\n",
    "\n",
    "嵌入是单词的向量表示,编码了语义含义。在检索增强生成中,嵌入有助于找到相关的外部知识来增强生成式聊天机器人 LLM 的能力。聊天机器人 LLM 在生成过程中使用检索到的嵌入来构建更加知识丰富和相关的响应。\n",
    "\n",
    "![](./images/Chatbot_lang.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a291d49",
   "metadata": {},
   "source": [
    "#### 设置向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491ef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 从现有的Chroma向量存储中加载现有的DeepRacer信息\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 将预先生成的嵌入加载到向量存储中。\n",
    "vectorstore_chromadb = Chroma(persist_directory=\"./persistent/chroma_db\", embedding_function=bedrock_embeddings_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a2f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 创建 AWS DeepRacer QA 链\n",
    "\n",
    "为了构建 RAG 链,我们使用了 LangChain 链中的一种,[Retrieval QA chain](https://python.langchain.com/docs/use_cases/question_answering/),它内置了对 RAG 的支持。\n",
    "\n",
    "提示使用了几种技术来确保与 AWS DeepRacer 相关的响应:\n",
    "- 从向量存储中检索到的文档被插入到 {context} 变量中,使其可供 LLM 使用\n",
    "- 通过要求 LLM 使用通过 RAG 检索到的信息来回答问题,减少幻觉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e9574-4f63-453e-be0e-3194f3b26bee",
   "metadata": {},
   "source": [
    "### 构建检索链\n",
    "\n",
    "为了确保我们能从 vector store 中检索到相关文档并生成合理的响应，我们需要：\n",
    "\n",
    "1. 创建一个 retriever\n",
    "2. 创建一个提示模板\n",
    "3. 构建检索链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105e0ec-5077-4342-b93e-0fc662ada42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(bedrock_llm_client, prompt)\n",
    "retriever = vectorstore_chromadb.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5b797-6894-43e5-b41d-52d53c40dbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 检查链路的响应\n",
    "retrieval_output = retrieval_chain.invoke({\"input\": \"What is DeepRacer?\"})\n",
    "\n",
    "# 输出完整的响应（这也将让我们看到输入和从 vector store 中获取的上下文）\n",
    "import pprint\n",
    "print(\"\\n== Full Response ==\\n\")\n",
    "pprint.pp(retrieval_output)\n",
    "\n",
    "# 英文答案摘要\n",
    "print(\"\\n== English Answer ==\\n\")\n",
    "print_ww(retrieval_output['answer'].strip())\n",
    "\n",
    "# 取答案部分，调用我们的翻译链，生成中文响应\n",
    "print(\"\\n== Chinese Answer ==\\n\")\n",
    "chinese = translation_chain.invoke({\"user_input\": retrieval_output['answer']})\n",
    "chinese = translation_format(chinese)\n",
    "print_ww(chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67693d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 我们一步一步地进行，使用我们的 Chroma 向量存储设置一个 RAG QA 链\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: Given an AWS DeepRacer related question, provide a detailed answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the human's next reply:\n",
    "<human_reply>\n",
    " {question}\n",
    "</human_reply>\n",
    "\n",
    "If the answer cannot be found in the context, the AI truthfully says it does not know the answer.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# 注意：我们正在使用一种较旧（已弃用）的方法来创建 QA 链和工具调用代理，\n",
    "# 因为最新的 'create_tool_calling_agent' 函数截至\n",
    "# 2024-05-08 还不支持 Bedrock 模型，因为缺少 .bind_tools 方法\n",
    "deepracer_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=bedrock_llm_client,\n",
    "    chain_type=\"stuff\",  # 添加所有返回的文档\n",
    "    retriever=vectorstore_chromadb.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3\n",
    "        },  # 从向量存储中返回前 3 个相似的文档\n",
    "    ),\n",
    "    # return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de31e4",
   "metadata": {},
   "source": [
    "#### 创建 LangChain 自定义工具\n",
    "\n",
    "现在我们将 AWS DeepRacer 问答链打包为 LangChain 自定义工具,并使其可供代理使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478de9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "# 初始化 LLM 工具\n",
    "deepracer_knowledge_tool = Tool.from_function(\n",
    "    name='AWS DeepRacer knowledge base',\n",
    "    func=deepracer_qa_chain.run,\n",
    "    description='Useful when you need to answer generic questions about AWS DeepRacer, Input: the question to answer, Output: Answer to the question',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50957832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "        deepracer_knowledge_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b4b15",
   "metadata": {},
   "source": [
    "## 使用 ReAct 框架:协同语言模型中的推理和行动\n",
    "大型语言模型可以交替生成对其推理的解释和特定任务的响应。\n",
    "\n",
    "生成推理解释使模型能够推断、监控和修改行动计划,甚至处理意外情况。行动步骤允许模型与外部知识库或环境等外部资源进行交互并从中获取信息。\n",
    "\n",
    "在这里,我们将初始化负责决定下一步行动的链。这由语言模型和提示驱动。该链的输入包括:\n",
    "\n",
    "- 可用工具列表\n",
    "- 用户输入 \n",
    "- 任何先前执行的步骤(中间步骤)\n",
    "\n",
    "然后,该链返回要执行的下一个动作,或要发送给用户的最终响应(AgentAction 或 AgentFinish)。\n",
    "\n",
    "不同的 [LangChain 代理类型](https://python.langchain.com/docs/modules/agents/agent_types/)具有不同的推理提示风格、不同的输入编码方式以及不同的输出解析方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53a38c-1c7a-4c5a-a274-e372c5aef6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "react_agent = initialize_agent(tools,\n",
    "                               llm=bedrock_llm_client,\n",
    "                               agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                               verbose=True, # displays the intermediate steps the agent performs\n",
    "                               return_intermediate_steps=True,\n",
    "                               #handle_parsing_errors=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a46ef2",
   "metadata": {},
   "source": [
    "让我们来看看这个链条使用的默认提示。\n",
    "\n",
    "请注意,提示模板具有以下结构:\n",
    "```\n",
    "\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{available tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of {tool names}\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\n",
    "```\n",
    "\n",
    "默认提示指导模型如何推理并与可用工具交互。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa0907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent.agent.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711eec1",
   "metadata": {},
   "source": [
    "一旦代理```react_agent```设置完成,我们就可以通过传递提示/问题来调用代理,如下所示。你可以随意调整后续调用中的提示/问题。\n",
    "\n",
    "当运行以下命令时,你看到代理如何开始推理它需要做什么来回答问题,它使用什么工具以及它们的输入。\n",
    "如果一切顺利,代理会产生一个最终答案。\n",
    "\n",
    "下面的问题,代理通常可以在一个计划->行动->观察循环中解决,你看到一个\"> Finished chain.\"和最终答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51337409",
   "metadata": {},
   "source": [
    "⚠️ **如果在运行下面的单元格时遇到访问被拒绝的问题,请检查Amazon Bedrock中Mixtral 8x7B和Titan Embeddings G1 – Text的状态是否都为Access granted。** ⚠️\n",
    "\n",
    "⚠️ **如果你遇到OutputParserException。这有时会发生,因为LLM没有以正确的格式返回\n",
    "问题的响应。我们将在后面的实验中通过引入few-shot提示来改进这一点,在提示中提供一些关于LLM如何响应的示例** ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c237474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 让我们尝试调用我们的新 RetrievalQA 链\n",
    "# 注意：这种调用代理的方法已被废弃，但由于 LangChain 为 Amazon Bedrock \n",
    "# 提供的封装中缺少一些函数调用功能，我们无法使用 LangChain 最近版本提供的创建检索链的较新方法。\n",
    "question = \"What is AWS DeepRacer?\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "print(\"\\n== English ==\\n\")\n",
    "print_ww(output)\n",
    "\n",
    "# 接下来，我们使用我们的翻译链帮助我们翻译代理的输出。\n",
    "chinese_output = translation_chain.invoke(output)\n",
    "chinese_output = translation_format(chinese_output)\n",
    "print(\"\\n== Chinese ==\\n\")\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4bc4b-fc9c-4012-9596-0078f97b6471",
   "metadata": {},
   "source": [
    "## 推理链条可能存在的问题\n",
    "\n",
    "此问题展示了使用代理时可能发生的一个问题。在解决这个问题时，代理通常会陷入一个循环，每次都是相同的思考和行动，直到它无法遵循其被指示的格式。\n",
    "\n",
    "我们将在实验室中通过引入少数示例提示来改进这一点，提供一些示例在提示中说明 LLM 如何推理，并让代理管理解析错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d7514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    question = \"How do I get started with training a DeepRacer model? anything specific I should think of?\"\n",
    "    output = react_agent(question)[\"output\"]\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nThe agent encountered an error! \\n {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3a981-612c-49a0-b5c4-dd6e353a9e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 接下来，我们使用我们的翻译链帮助我们翻译代理的输出。\n",
    "chinese_output = translation_chain.invoke(output)\n",
    "chinese_output = translation_format(chinese_output)\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd9892-e577-4852-8b2e-98c59b2fac6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=bedrock_llm_client, verbose=True, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19396",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 为 LLM 添加更多工具以提升其分析 DeepRacer 模型的能力\n",
    "\n",
    "到目前为止,LLM 只能回答关于 AWS DeepRacer 的一般信息,这些信息是 [AWS DeepRacer 开发人员指南](https://docs.aws.amazon.com/deepracer/latest/developerguide/what-is-deepracer.html)的一部分。\n",
    "在本节中,我们将把 LLM 与 AWS DeepRacer 服务 API 集成,以便 LLM 可以与您在此 AWS 账户中有权访问的不同模型进行交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9b033",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 创建一个列出 AWS DeepRacer 模型的工具\n",
    "\n",
    "当代理调用此工具时,它将列出位于 AWS DeepRacer 服务中的可用 DeepRacer 模型。它已经作为一个 LangChain 自定义工具实现,并在代理调用时使用自定义代码来调用 DeepRacer 服务 API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76ea86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "class DeepRacerListModelsTool(BaseTool):\n",
    "    name = \"List AWS DeepRacer models\"\n",
    "    description = \"\"\"\n",
    "    Use this tool when you need to list all AWS DeepRacer models. Lists the name of each DeepRacer model.\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self, input=None):\n",
    "        # List all available DeepRacer models\n",
    "        models = deepracer_local.list_models()\n",
    "        return models\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca56892",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 创建一个获取 AWS DeepRacer 模型详细信息的工具\n",
    "\n",
    "下载一个 AWS DeepRacer 模型并提取相关部分,以允许 LLM 对其训练和性能进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff719b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ModelAnalysisInput(BaseModel):\n",
    "    model_name: str = Field()\n",
    "\n",
    "class DeepRacerModelAnalysisTool(BaseTool):\n",
    "    name = \"AWS DeepRacer model details\"\n",
    "    description = \"\"\"Use this tool to get detailed information about an AWS DeepRacer model. Input: Expects the model_name as string input. Output: Python dictionary. IMPORTANT: DeepRacer track names are preceded by 'WORLD_NAME:'\"\"\"\n",
    "    args_schema = ModelAnalysisInput\n",
    "\n",
    "    def _run(self, model_name=None):\n",
    "        if deepracer_local.model_exists(model_name):\n",
    "            # Attempt to load model details from the local `deepracer_models` folder\n",
    "            # using helpers in `utils/deepracer_local.py`\n",
    "            model_data = {}\n",
    "            \n",
    "            model_data[\"model_metadata_used_for_training\"] = deepracer_local.get_model_metadata(model_name)\n",
    "            model_data[\"reward_function_used_for_training\"] = deepracer_local.get_reward_function(model_name)\n",
    "            model_data[\"hyper_parameters_used_for_training\"] = deepracer_local.get_hyper_parameters(model_name)\n",
    "            model_data[\"evaluation_results\"] = deepracer_local.get_evaluation_metrics(model_name)\n",
    "            \n",
    "            # We exclude the training results here because they tend to be very long (and Mixtral 8x7B only has a 32K context window)\n",
    "            #model_data[\"training_results\"] = deepracer_local.get_training_metrics(model_name)\n",
    "            \n",
    "            model_data[\"evaluation_parameters\"] = deepracer_local.get_evaluation_params(model_name)\n",
    "            model_data[\"training_parameters\"] = deepracer_local.get_training_params(model_name)\n",
    "        \n",
    "            # We hard-code the \"track metadata\", as the original function in the DeepRacer\n",
    "            # GenAI lab simply responded with a hard-coded string\n",
    "            model_data[\"track_meta_data\"] =  \"Track difficulty is an integer ranging from 100 to 1, where 1 is the hardest\"\n",
    "        \n",
    "            return model_data\n",
    "        else:\n",
    "            return f\"Model with name {model_name} does not exist.\"\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55a075",
   "metadata": {},
   "source": [
    "在创建了第三个也是最后一个工具后,我们将这 3 个工具添加到一个列表中,该列表将作为位置参数传递给代理初始化函数。这使得代理在解析提示时可以使用这些工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57058eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "        DeepRacerModelAnalysisTool(),\n",
    "        DeepRacerListModelsTool(),\n",
    "        deepracer_knowledge_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022844c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "react_agent = initialize_agent(tools,\n",
    "                               llm=bedrock_llm_client,\n",
    "                               agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                               verbose=True, # verbose logs is turned on to give insights into how the agent will reason and use the tools\n",
    "                               return_intermediate_steps=True,\n",
    "                               handle_parsing_errors=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b72fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above.\n",
    "Action: the action to take, should be one of {tools}.\n",
    "Action Input: the input to the action.\n",
    "Observation: the result of the action.\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: the final answer to the original input question.\n",
    "\n",
    "<examples>\n",
    "<example>\n",
    "Question: What is AWS DeepRacer?\n",
    "Thought: I need to get more information on what AWS DeepRacer is, I should check the AWS DeepRacer knowledge base\n",
    "Action: AWS DeepRacer knowledge base\n",
    "Action Input: What is AWS DeepRacer?\n",
    "Observation:  AWS DeepRacer is the fastest way to get rolling with reinforcement learning (RL), literally, with a fully autonomous 1/18th scale race car driven by reinforcement learning, 3D racing simulator, and a global racing league.\n",
    "Final Answer: AWS DeepRacer is the fastest way to get rolling with reinforcement learning (RL), literally, with a fully autonomous 1/18th scale race car driven by reinforcement learning, 3D racing simulator, and a global racing league.\n",
    "</example>\n",
    "<example>\n",
    "Question: how can I improve my-deepracer-model?\n",
    "Thought: I need to check the details and performance of the AWS DeepRacer model to answer this question.\n",
    "Action: AWS DeepRacer model details\n",
    "Action Input: my-deepracer-model\n",
    "Observation: ....\n",
    "Thought: I should check the AWS DeepRacer knowledge base on how to improve a DeepRacer model\n",
    "Action: AWS DeepRacer knowledge base\n",
    "Action Input: Improve deepracer model\n",
    "Observation: ....\n",
    "Thought: With the details about the model and knowledge on how to improve a model I know have enough information.\n",
    "Final Answer: The my-deepracer-model model is designed and trained to follow the center line of the track. The model details, reward function, and hyperparameters used for training provide evidence that this model prioritizes staying on the center line, making it a good performing model for that task.\n",
    "</example>\n",
    "</examples>\n",
    "\n",
    "You are an AWS DeepRacer Q&A bot, give a detailed answer to the question.\n",
    "You can only answer question not perform any changes to any DeepRacer models.\n",
    "Question: {input}\n",
    "\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "# Create a prompt template and inject the names of all available tools in the {tools} variable. This can be done using partial_variables\n",
    "tool_names = json.dumps([d.name for d in tools])\n",
    "template = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    partial_variables={\"tools\": tool_names},\n",
    ")\n",
    "react_agent.agent.llm_chain.prompt = template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100d3b9",
   "metadata": {},
   "source": [
    "通过添加 DeepRacerListModelsTool 和 DeepRacerModelAnalysisTool 自定义工具,代理现在能够:\n",
    "1. 列出可用的 AWS DeepRacer 模型\n",
    "2. 下载指定的 DeepRacer 模型并提取:\n",
    "     - 训练数据 \n",
    "     - 评估数据\n",
    "     - 提取模型元数据,如动作空间、超参数等\n",
    "     - 提取奖励函数\n",
    "\n",
    "正如我们在实验室前面所说,代理可以访问此 AWS 账户中的一些 DeepRacer 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936bc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(f\"Name: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf18c9",
   "metadata": {},
   "source": [
    "在向代理提问时可以使用这些 DeepRacer 模型名称。\n",
    "\n",
    "在这个问题中,代理将使用 **List AWS DeepRacer models** 工具通过 DeepRacer 服务 API 获取当前 AWS 账户中 DeepRacer 模型的名称。在 **Finished chain** 之后是返回给用户的输出。\n",
    "\n",
    "在 **Finished chain** 之前看到的文本只是由于我们为代理使用的 LLM 打开了 **verbose=True** 而产生的日志。这是为了更好地突出代理如何推理和与工具交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3ecef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the names of DeepRacer models I have access to?\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "\n",
    "print('\\n== English ==\\n')\n",
    "print_ww(output) # Print the model's output\n",
    "\n",
    "chinese_output = translation_chain.invoke({\"user_input\": output})\n",
    "chinese_output = translation_format(chinese_output) # Format output nicely\n",
    "print('\\n== Chinese ==\\n')\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147fbbf",
   "metadata": {},
   "source": [
    "## 下一步：考虑模型细节\n",
    "\n",
    "在这个问题中,代理将使用 **AWS DeepRacer model details** 工具通过 DeepRacer 服务 API 提取 **MODEL_NAME** 模型详细信息(超参数、奖励函数、训练指标、评估指标和使用的赛道)。然后将这些数据作为 json 返回给代理,代理将搜索评估使用的是哪个赛道。\n",
    "\n",
    "在 **Finished chain** 之后是返回给用户的输出。\n",
    "\n",
    "将 MODEL_NAME 变量更改为您想要了解的模型的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4f65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"AtoZ-CCW-Centerline\"\n",
    "COMPARE_MODEL_NAME=\"AtoZ-CCW-Steering-Penalty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43d52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Which track was \" + MODEL_NAME + \" trained and evaluated on?\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "\n",
    "print('\\n== English ==\\n')\n",
    "print_ww(output) # Print the model's output\n",
    "\n",
    "chinese_output = translation_chain.invoke({\"user_input\": output})\n",
    "chinese_output = translation_format(chinese_output) # Format output nicely\n",
    "print('\\n== Chinese ==\\n')\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe1b36",
   "metadata": {},
   "source": [
    "## 让我们尝试更多的问题,看看代理如何推理和使用工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Is \" + MODEL_NAME + \" or \" + COMPARE_MODEL_NAME + \" the better model? please also state why\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "\n",
    "print('\\n== English ==\\n')\n",
    "print_ww(output) # Print the model's output\n",
    "\n",
    "chinese_output = translation_chain.invoke({\"user_input\": output})\n",
    "chinese_output = translation_format(chinese_output) # Format output nicely\n",
    "print('\\n== Chinese ==\\n')\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34447e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How can I improve the training for \" + MODEL_NAME + \"?\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "\n",
    "print('\\n== English ==\\n')\n",
    "print_ww(output) # Print the model's output\n",
    "\n",
    "chinese_output = translation_chain.invoke({\"user_input\": output})\n",
    "chinese_output = translation_format(chinese_output) # Format output nicely\n",
    "print('\\n== Chinese ==\\n')\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d8cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Should I change any of the hyperparameters that was used to train \" + MODEL_NAME + \" to improve the training?\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "\n",
    "print('\\n== English ==\\n')\n",
    "print_ww(output) # Print the model's output\n",
    "\n",
    "chinese_output = translation_chain.invoke({\"user_input\": output})\n",
    "chinese_output = translation_format(chinese_output) # Format output nicely\n",
    "print('\\n== Chinese ==\\n')\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6001e-5563-4e25-b3b3-7da60a96626f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Write a simple deepracer reward function for me\"\n",
    "output = react_agent(question)[\"output\"]\n",
    "\n",
    "print('\\n== English ==\\n')\n",
    "print_ww(output) # Print the model's output\n",
    "\n",
    "# NOTE: We disable the formatter here because the characters it searches for may be \n",
    "# present in any Python code the model generates\n",
    "chinese_output = translation_chain.invoke({\"user_input\": output})\n",
    "#chinese_output = translation_format(chinese_output) # Format output nicely\n",
    "print('\\n== Chinese ==\\n')\n",
    "print_ww(chinese_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d0c68",
   "metadata": {},
   "source": [
    "# 回顾\n",
    "\n",
    "在本实验中,我们使用自定义工具将 LangChain 代理与 AWS DeepRacer 服务集成在一起。这使得 LLM 能够帮助我们进行模型分析以及如何改进模型。\n",
    "\n",
    "\n",
    "### 后续步骤:\n",
    "[Amazon Bedrock 研讨会](https://github.com/aws-samples/amazon-bedrock-workshop) - 更多使用案例和使用 Amazon Bedrock 构建的方法\n",
    "\n",
    "[SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html) - 为各种问题类型提供预训练的开源模型,帮助您开始机器学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ddaf3-e0ee-46ff-b812-db9ce09aa82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
